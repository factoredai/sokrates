{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import dill\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC, GradientBoostingClassifier as GBM\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "Notebook to create models (3 inputs) and explainers and save them to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"Dataset\"\n",
    "\n",
    "df = pd.concat([\n",
    "    pd.read_csv(os.path.join(datapath, p))\n",
    "    for p in os.listdir(datapath)\n",
    "    if p.endswith(\".csv\")\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "df.body = df.body.str.lower()\n",
    "df.title = df.title.str.lower()\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.quantile(df[\"word_count\"], [0.005, 0.5, 0.9, 0.95]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_score = df.score.median()\n",
    "print(\"Score median: %0.4f\" % median_score)\n",
    "df[\"target\"] = df[\"score\"] >= median_score\n",
    "\n",
    "print(\"Target Mean: %0.4f\" % df[\"target\"].mean())\n",
    "\n",
    "FEATURES = [\n",
    "    \"wh_word_count\",\n",
    "    \"sentence_count\",\n",
    "    \"word_count\",\n",
    "    \"example_count\",\n",
    "    \"n_linebreaks\",\n",
    "    \"title_word_count\",\n",
    "    \"title_question_marks\",\n",
    "    \"num_question_marks\",\n",
    "    \"n_links\",\n",
    "#     \"n_tags\",\n",
    "    \"n_lists\",\n",
    "]\n",
    "\n",
    "x = df[FEATURES + [\"body\", \"title\"]]\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def make_tokenizer(text_vecs: List[List[str]], *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a tokenizer on the given lists of strings (corpus).\n",
    "    \"\"\"\n",
    "    texts = [\" \".join(row) for row in zip(*text_vecs)]\n",
    "    tok = keras.preprocessing.text.Tokenizer(*args, **kwargs)\n",
    "    tok.fit_on_texts(texts)\n",
    "    return tok\n",
    "\n",
    "\n",
    "\n",
    "def make_model(\n",
    "        max_body_len: int,\n",
    "        max_title_len: int,\n",
    "        vocab_size: int,\n",
    "        num_handmade_feat: int,\n",
    "        emb_dim: int = 64,\n",
    "        dropout_rate: float = 0.4):\n",
    "    \"\"\"\n",
    "    Makes the keras model. Define model architecture here.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define inputs\n",
    "    body_input = keras.layers.Input((max_body_len,), name=\"body_tokens\")\n",
    "    title_input = keras.layers.Input((max_title_len,), name=\"title_tokens\")\n",
    "    feature_inputs = keras.layers.Input((num_handmade_feat,), name=\"features_input\")\n",
    "    \n",
    "    # Embeddings\n",
    "    embedding = keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=emb_dim,\n",
    "        name=\"word_embed\"\n",
    "    )\n",
    "    \n",
    "    # Process title and body texts\n",
    "    title_emb = embedding(title_input)\n",
    "    title_emb = keras.layers.Dropout(dropout_rate)(title_emb)\n",
    "    title_gru = keras.layers.Bidirectional(keras.layers.GRU(16, activation=\"tanh\"))(title_emb)\n",
    "    title_gru = keras.layers.Dropout(dropout_rate)(title_gru)\n",
    "    \n",
    "    body_emb = embedding(body_input)\n",
    "    body_emb = keras.layers.Dropout(dropout_rate)(body_emb)\n",
    "    body_gru = keras.layers.Bidirectional(keras.layers.GRU(96, activation=\"tanh\"))(body_emb)\n",
    "    body_gru = keras.layers.Dropout(dropout_rate)(body_gru)\n",
    "    \n",
    "    # Combine features\n",
    "    all_feat = keras.layers.Concatenate(axis=1)([title_gru, body_gru, feature_inputs])\n",
    "    all_feat = keras.layers.BatchNormalization()(all_feat)\n",
    "    \n",
    "    # Final layers\n",
    "    dense_1 = keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(1e-4))(all_feat)\n",
    "    dense_1 = keras.layers.Dropout(dropout_rate)(dense_1)\n",
    "    \n",
    "    output = keras.layers.Dense(1, activation=\"sigmoid\")(dense_1)\n",
    "    return keras.Model(inputs=[title_input, body_input, feature_inputs], outputs=output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(854)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "train_bodies = xtrain[\"body\"]\n",
    "train_titles = xtrain[\"title\"]\n",
    "\n",
    "test_bodies = xtest[\"body\"]\n",
    "test_titles = xtest[\"title\"]\n",
    "\n",
    "print(xtrain.shape, xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 7000\n",
    "MAX_BODY_LEN = 180\n",
    "MAX_TITLE_LEN = 24\n",
    "NUM_HANDMADE = len(FEATURES)\n",
    "EMB_DIM = 64\n",
    "\n",
    "\n",
    "def df_to_inputs(df: pd.DataFrame, tokenizer):\n",
    "    \"\"\"\n",
    "    Converts a df to the inputs required by the model \n",
    "    (title, body, hand-engineered features).\n",
    "    \"\"\"\n",
    "    bodies = keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences(df[\"body\"]),\n",
    "        maxlen=MAX_BODY_LEN,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\"\n",
    "    )\n",
    "    \n",
    "    titles = keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences(df[\"title\"]),\n",
    "        maxlen=MAX_TITLE_LEN,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\"\n",
    "    )\n",
    "    \n",
    "    return titles, bodies, df[FEATURES].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tokenizer\n",
    "tokenize = make_tokenizer([train_bodies, train_titles], oov_token=\"<oov>\", num_words=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear backend\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Make model\n",
    "model = make_model(\n",
    "    MAX_BODY_LEN,\n",
    "    MAX_TITLE_LEN,\n",
    "    VOCAB_SIZE,\n",
    "    NUM_HANDMADE,\n",
    "    EMB_DIM,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "train_inputs = df_to_inputs(xtrain, tokenize)\n",
    "test_inputs = df_to_inputs(xtest, tokenize)\n",
    "\n",
    "train_inputs[-1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_inputs, ytrain, validation_data=(test_inputs, ytest), epochs=20, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(train_inputs) > 0.5\n",
    "\n",
    "train_acc = accuracy_score(y_true=ytrain, y_pred=train_pred)\n",
    "train_mat = confusion_matrix(y_true=ytrain, y_pred=train_pred)\n",
    "\n",
    "print(\"Train set performance:\")\n",
    "print(\"Accuracy: %0.4f\" % train_acc)\n",
    "print(\"Confusion matrix: \\n\", train_mat)\n",
    "print()\n",
    "\n",
    "test_pred = model.predict(test_inputs) > 0.5\n",
    "\n",
    "test_acc = accuracy_score(y_true=ytest, y_pred=test_pred)\n",
    "test_mat = confusion_matrix(y_true=ytest, y_pred=test_pred)\n",
    "\n",
    "print(\"Test set performance:\")\n",
    "print(\"Accuracy: %0.4f\" % test_acc)\n",
    "print(\"Confusion matrix: \\n\", test_mat)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13, 8))\n",
    "\n",
    "axs[0].plot(hist.history[\"loss\"], color=\"blue\", label=\"Train Loss\")\n",
    "axs[0].plot(hist.history[\"val_loss\"], color=\"red\", label=\"Val Loss\")\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Loss Function\")\n",
    "\n",
    "axs[1].plot(hist.history[\"accuracy\"], color=\"blue\", label=\"Train Acc\")\n",
    "axs[1].plot(hist.history[\"val_accuracy\"], color=\"red\", label=\"Val Acc\")\n",
    "axs[1].legend()\n",
    "axs[1].set_title(\"Accuracy\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "explainer = LimeTabularExplainer(train_inputs[-1].values, feature_names=list(train_inputs[-1].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dir(save_dir: str = \"models\") -> str:\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    store = os.path.join(save_dir, today)\n",
    "    \n",
    "    if not os.path.isdir(store):\n",
    "        os.makedirs(store)\n",
    "    \n",
    "    number = len(os.listdir(store))\n",
    "    store = os.path.join(store, f\"model_{number:02d}\")\n",
    "    os.mkdir(store)\n",
    "    return store\n",
    "\n",
    "def save_model(model, tokenizer, explainer=None, meta: dict = {}, save_dir: str = \"models\"):\n",
    "    \"\"\"\n",
    "    Save model, tokenizer and Lime explainer.\n",
    "    \"\"\"\n",
    "    store = make_dir(save_dir)\n",
    "    model.save(os.path.join(store, \"model.h5\"))\n",
    "    \n",
    "    with open(os.path.join(store, \"tokenizer.json\"), \"w\") as f:\n",
    "        f.write(tokenizer.to_json())\n",
    "    \n",
    "    meta.update(\n",
    "        num_inputs=len(model.inputs),\n",
    "        body_pad_length=MAX_BODY_LEN,\n",
    "        title_pad_length=MAX_TITLE_LEN,\n",
    "        features=FEATURES\n",
    "    )\n",
    "    meta_path = os.path.join(store, \"meta.json\")\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "    \n",
    "    if explainer is not None:\n",
    "        exp_path = os.path.join(store, \"explainer.dill\")\n",
    "        with open(exp_path, \"wb\") as f:\n",
    "             dill.dump(explainer, f)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model, tokenize, explainer, meta={\"val_accuracy\": float(test_acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: models/2020-12-01/model_01/explainer.dill to s3://models-storage-dsr/models/2020-12-01/model_01/explainer.dill\n",
      "upload: models/2020-12-01/model_01/meta.json to s3://models-storage-dsr/models/2020-12-01/model_01/meta.json\n",
      "upload: models/2020-12-01/model_01/model.h5 to s3://models-storage-dsr/models/2020-12-01/model_01/model.h5\n",
      "upload: models/2020-12-01/model_01/tokenizer.json to s3://models-storage-dsr/models/2020-12-01/model_01/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync models/ s3://models-storage-dsr/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
